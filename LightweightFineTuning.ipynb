{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Loading and Evaluating a Foundation Model](#lefm)\n",
    "2. [Performing PEFT](#ppeft)\n",
    "3. [Performing Inference with a PEFT Model](#piwpeft)\n",
    "\n",
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: GPT-2\n",
    "* Evaluation approach: Hugging Face `Trainer`\n",
    "* Fine-tuning dataset: imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "<a id=\"lefm\"></a>\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dbedb",
   "metadata": {},
   "source": [
    "Load stanfordnlp/imdb dataset, which is for text classification, split it into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 1000\n",
       " })}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "ds = {split: ds for split, ds in zip(splits, load_dataset(\"stanfordnlp/imdb\", split=splits))}\n",
    "\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8099d2256d24a129738786c191444a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72d0349ba684559a2e051ee4761606c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73350eebec60451cb1307f003a59f842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192c598cf1514ee49400554bbacb7099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c76bb652e5f47228c5fda9aa6fbc781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2433b49c4768453684decbf4c8a47370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815fd736513f4ce6aadd6345effdee5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 1000\n",
       " })}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token if it's not already set, \n",
    "#  or it will throw error when tokenize the inputs\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "\n",
    "    \n",
    "# The Trainer needs this column to compute the loss. We only remove the original 'text' column\n",
    "# which is now redundant after tokenization.\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = tokenized_ds[split].rename_column(\"label\", \"labels\")\n",
    "    tokenized_ds[split] = tokenized_ds[split].remove_columns(['text'])   \n",
    "\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24a594a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bd0e1a8cc24c518fd37f914398ac91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        # The model is loaded in 8-bit to reduce memory usage.\n",
    "        # load_in_8bit=True, \n",
    "        num_labels=2,  # IMDB has 2 labels: positive and negative\n",
    "        device_map=\"auto\" # Automatically map model layers to available devices (CPU/GPU)\n",
    "    )\n",
    "\n",
    "# GPT-2's architecture doesn't have a pad token, so we need to tell the model\n",
    "# which token to use for padding. We'll use the end-of-sentence token.\n",
    "# foundation_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "# Default Behavior: By default, the Trainer.evaluate() method automatically calculates the evaluation loss (eval_loss). For a Causal Language Model like GPT-2, this loss is the primary metric. It measures how well the model predicts the next token in the sequence.\n",
    "# When to Use compute_metrics: You would typically provide a compute_metrics function when you want to calculate metrics other than loss, such as accuracy, precision, recall, or F1-score. This is most common for classification tasks (e.g., using AutoModelForSequenceClassification), where the model makes a distinct prediction that can be directly compared to a true label\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# mlm=False indicates that we are doing Causal Language Modeling (next token prediction), not Masked Language Modeling.\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "foundation_trainer = Trainer(\n",
    "    model=foundation_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis\",\n",
    "        learning_rate=2e-5,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 01:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.962777853012085,\n",
       " 'eval_accuracy': 0.488,\n",
       " 'eval_runtime': 85.0784,\n",
       " 'eval_samples_per_second': 11.754,\n",
       " 'eval_steps_per_second': 11.754}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate using trainer\n",
    "baseline_eval_results = foundation_trainer.evaluate()\n",
    "\n",
    "baseline_eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "<a id=\"ppeft\"></a>\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# Define LoRA configuration. It's crucial to set the task_type.\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    # We target 'c_attn' which is the attention layer in GPT-2's architecture.\n",
    "    # This is the most impactful layer for LoRA adaptation.\n",
    "    # If don't specify target_modules, PEFT will try to guess which linear layers to \n",
    "    #  apply LoRA to — but that doesn’t always work, especially for models like GPT2, \n",
    "    #  which have non-standard naming or custom layer wrappers.\n",
    "    # If LoRA modules aren’t properly injected, you’re essentially training nothing — \n",
    "    #  the added parameters aren't wired into the forward pass.\n",
    "    target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\" # This is essential for the Trainer to work correctly.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model = get_peft_model(foundation_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 126,802,176 || trainable%: 1.860611603384472\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8970fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "checkpoint_dir = \"/workspace/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_latest.pth\")\n",
    "torch.save(lora_model.state_dict(), checkpoint_path)\n",
    "\n",
    "# Keep only the last 3 checkpoints\n",
    "checkpoints = sorted(os.listdir(checkpoint_dir), reverse=True)\n",
    "if len(checkpoints) > 3:\n",
    "    os.remove(os.path.join(checkpoint_dir, checkpoints[-1]))  # Delete the oldest checkpoint\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "363cf63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "with open('/workspace/checkpoints/checkpoint_latest.pth', 'rb') as f_in:\n",
    "    with gzip.open('/workspace/checkpoints/checkpoint_latest.pth.gz', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e0e998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 24:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.948700</td>\n",
       "      <td>0.824730</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.090300</td>\n",
       "      <td>0.793243</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.818500</td>\n",
       "      <td>0.656786</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.756300</td>\n",
       "      <td>0.577490</td>\n",
       "      <td>0.816000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/lora_analysis/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/lora_analysis/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/lora_analysis/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4000, training_loss=0.959916259765625, metrics={'train_runtime': 1462.8614, 'train_samples_per_second': 2.734, 'train_steps_per_second': 2.734, 'total_flos': 2148393811968000.0, 'train_loss': 0.959916259765625, 'epoch': 4.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/lora_analysis\",\n",
    "        learning_rate=2e-5,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "<a id=\"smitd\"></a>\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tmp/yyan-peft-lora-gpt2/tokenizer_config.json',\n",
       " '/tmp/yyan-peft-lora-gpt2/special_tokens_map.json',\n",
       " '/tmp/yyan-peft-lora-gpt2/vocab.json',\n",
       " '/tmp/yyan-peft-lora-gpt2/merges.txt',\n",
       " '/tmp/yyan-peft-lora-gpt2/added_tokens.json',\n",
       " '/tmp/yyan-peft-lora-gpt2/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "# only saves the trained LoRA adapter weights,\n",
    "lora_model.save_pretrained(\"/tmp/yyan-peft-lora-gpt2\")\n",
    "\n",
    "# need to also save the tokenizer separately into the same directory. The AutoPeftModelForCausalLM class is smart enough to load the base model and then apply the adapter on top, but you still need to load the tokenizer from a complete configuration.\n",
    "tokenizer.save_pretrained(\"/tmp/yyan-peft-lora-gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "<a id=\"piwpeft\"></a>\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "reloaded_model = AutoPeftModelForSequenceClassification.from_pretrained(\"/tmp/yyan-peft-lora-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/tmp/yyan-peft-lora-gpt2\")\n",
    "# inputs = tokenizer(\"Hello, my name is \", return_tensors=\"pt\")\n",
    "# outputs = reloaded_model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "# print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 01:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: {'eval_loss': 2.962777853012085, 'eval_accuracy': 0.488, 'eval_runtime': 85.0784, 'eval_samples_per_second': 11.754, 'eval_steps_per_second': 11.754}\n",
      "Fine-Tuned Model: {'eval_loss': 0.5774895548820496, 'eval_accuracy': 0.816, 'eval_runtime': 103.3878, 'eval_samples_per_second': 9.672, 'eval_steps_per_second': 9.672, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_performance = lora_trainer.evaluate()\n",
    "print(\"Original Model:\", baseline_eval_results)\n",
    "print(\"Fine-Tuned Model:\", fine_tuned_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adapter_model.bin',\n",
       " 'adapter_config.json',\n",
       " 'README.md',\n",
       " 'merges.txt',\n",
       " 'tokenizer.json',\n",
       " 'tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'vocab.json']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/tmp/yyan-peft-lora-gpt2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/checkpoints\n"
     ]
    }
   ],
   "source": [
    "cd /workspace/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99961024",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'du' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdu\u001b[49m \u001b[38;5;241m-\u001b[39msh\n",
      "\u001b[0;31mNameError\u001b[0m: name 'du' is not defined"
     ]
    }
   ],
   "source": [
    "du -sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1119e996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48\r\n",
      "drwxr-xr-x 5 student student  4096 Aug 13 21:14 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 1 root    root     4096 Aug 13 21:14 \u001b[01;34m..\u001b[0m/\r\n",
      "drwxr-xr-x 2 student student  4096 Aug 13 21:14 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\r\n",
      "-rw-r--r-- 1 student student    61 Aug 13 21:07 .workspace-submit.json\r\n",
      "-rw-r--r-- 1 student student 20654 Aug 13 21:08 LightweightFineTuning.ipynb\r\n",
      "drwxr-xr-x 2 student student  4096 Aug 13 21:14 \u001b[01;34mcheckpoints\u001b[0m/\r\n",
      "drwxr-xr-x 5 student student  4096 Aug 13 21:14 \u001b[01;34mdata\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac21d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 929M\r\n",
      "-rw-r--r-- 1 student student 484M Aug 14 13:37 checkpoint_latest.pth\r\n",
      "-rw-r--r-- 1 student student 445M Aug 14 13:38 checkpoint_latest.pth.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5af0345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"./checkpoint_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e1dc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 445M\r\n",
      "-rw-r--r-- 1 student student 445M Aug 14 13:38 checkpoint_latest.pth.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebd59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
