{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lightweight Fine-Tuning Project",
   "id": "b08d6bb61c92a752"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: GPT-2\n",
    "* Evaluation approach: Hugging Face `Trainer`\n",
    "* Fine-tuning dataset: imdb"
   ],
   "id": "e2e9198249a8c051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ],
   "id": "34610a7eda851ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T16:41:14.288049Z",
     "start_time": "2025-08-13T16:41:06.589693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "ds = {split: ds for split, ds in zip(splits, load_dataset(\"stanfordnlp/imdb\", split=splits))}\n",
    "\n",
    "for split in splits:\n",
    "    ds[split] = ds[split].shuffle(seed=42).select(range(500))\n",
    "\n",
    "ds"
   ],
   "id": "cad4f6ec594648dd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanyan/personal-github/workspace-udacity/generative-ai-nanodegree/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 471912.76 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 639613.51 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 591842.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T16:42:01.278364Z",
     "start_time": "2025-08-13T16:41:54.741317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token if it's not already set,\n",
    "#  or it will throw error when tokenize the inputs\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "# **FIX**: Remove the original columns ('text', 'label') after tokenization.\n",
    "# The Trainer will pass all columns as arguments to the model, causing an error\n",
    "# if a column name doesn't match a model argument.\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = tokenized_ds[split].remove_columns(['text', 'label'])\n",
    "\n",
    "tokenized_ds"
   ],
   "id": "c720b68211ff638c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3673.90 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3262.10 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 500\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['input_ids', 'attention_mask'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T16:42:21.421117Z",
     "start_time": "2025-08-13T16:42:06.095294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    # The model is loaded in 8-bit to reduce memory usage.\n",
    "    # load_in_8bit=True,\n",
    "    device_map=\"auto\" # Automatically map model layers to available devices (CPU/GPU)\n",
    ")"
   ],
   "id": "72876d94c034ac61",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Default Behavior: By default, the Trainer.evaluate() method automatically calculates the evaluation loss (eval_loss). For a Causal Language Model like GPT-2, this loss is the primary metric. It measures how well the model predicts the next token in the sequence.\n",
    "# When to Use compute_metrics: You would typically provide a compute_metrics function when you want to calculate metrics other than loss, such as accuracy, precision, recall, or F1-score. This is most common for classification tasks (e.g., using AutoModelForSequenceClassification), where the model makes a distinct prediction that can be directly compared to a true label\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "# mlm=False indicates that we are doing Causal Language Modeling (next token prediction), not Masked Language Modeling.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "foundation_trainer = Trainer(\n",
    "    model=foundation_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #     compute_metrics=compute_metrics,\n",
    ")"
   ],
   "id": "51eadf8152717e3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# evaluate using trainer\n",
    "baseline_eval_results = foundation_trainer.evaluate()\n",
    "\n",
    "baseline_eval_results"
   ],
   "id": "9364febfbe013bd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ],
   "id": "eac302f157edd419"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# Define LoRA configuration. It's crucial to set the task_type.\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    #     target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # This is essential for the Trainer to work correctly based on the chosen foundation model: gpt2.\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ],
   "id": "d8c07830eac0efde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "lora_model.print_trainable_parameters()"
   ],
   "id": "d718e06269461882"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# optional\n",
    "import os\n",
    "import torch\n",
    "\n",
    "checkpoint_dir = \"/workspace/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_latest.pth\")\n",
    "torch.save(lora_model.state_dict(), checkpoint_path)\n",
    "\n",
    "# Keep only the last 3 checkpoints\n",
    "checkpoints = sorted(os.listdir(checkpoint_dir), reverse=True)\n",
    "if len(checkpoints) > 3:\n",
    "    os.remove(os.path.join(checkpoint_dir, checkpoints[-1]))  # Delete the oldest checkpoint"
   ],
   "id": "1dd109426511c089"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/lora_analysis\",\n",
    "        learning_rate=2e-3,\n",
    "        # Reduce the batch size if you don't have enough memory\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #     compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "lora_trainer.train()"
   ],
   "id": "38135572dfe68196"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ],
   "id": "902dca9671fa17ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# only saves the trained LoRA adapter weights,\n",
    "lora_model.save_pretrained(\"/tmp/yyan-peft-lora-gpt2\")\n",
    "\n",
    "# need to also save the tokenizer separately into the same directory. The AutoPeftModelForCausalLM class is smart enough to load the base model and then apply the adapter on top, but you still need to load the tokenizer from a complete configuration.\n",
    "tokenizer.save_pretrained(\"/tmp/yyan-peft-lora-gpt2\")"
   ],
   "id": "7f5812f145d53b50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ],
   "id": "7f080a3bd516d591"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "reloaded_model = AutoPeftModelForCausalLM.from_pretrained(\"/tmp/yyan-peft-lora-gpt2\")"
   ],
   "id": "e96afa9d45b9242b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/tmp/yyan-peft-lora-gpt2\")\n",
    "inputs = tokenizer(\"Hello, my name is \", return_tensors=\"pt\")\n",
    "outputs = reloaded_model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(outputs))"
   ],
   "id": "944e29975443dd6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fine_tuned_performance = lora_trainer.evaluate()\n",
    "print(\"Original Model:\", baseline_eval_results)\n",
    "print(\"Fine-Tuned Model:\", fine_tuned_performance)"
   ],
   "id": "bbb6c08f9281d759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.listdir(\"/tmp/yyan-peft-lora-gpt2/\")",
   "id": "654cf8518f581612"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
